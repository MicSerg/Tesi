from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from autocorrect import Speller
import pandas as pd
import numpy as np
from multiprocessing import Pool
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV
from time import time
import pickle
import datetime

n_samples = 2000
n_features = 2000
n_components = 20  # N Topics
n_top_words = 20


def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        message = "Topic #%d: " % topic_idx
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
    print()


apps_df = pd.read_csv('data/processed/prova.csv', delimiter=';', low_memory=False)
apps_df = apps_df.dropna()

print("Extracting tf features for LDA...")
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                                max_features=n_features)

t0 = time()
tf = tf_vectorizer.fit_transform(apps_df['description_processed'])
print("done in {}".format(time() - t0))

# <codecell> Grid search parameters

t0 = time()

# Define Search Param
search_params = {'n_components': [100], 'learning_decay': [.3]}

# Init the Model
lda = LatentDirichletAllocation(max_iter=50)

# Init Grid Search Class
model = GridSearchCV(lda, param_grid=search_params)

# Do the Grid Search
model.fit(tf)

# Best Model
best_lda_model = model.best_estimator_

# Model Parameters
print("Best Model's Params: ", model.best_params_)

# Log Likelihood Score -> Higher the better
print("Best Log Likelihood Score: {} (higher is better)".format(model.best_score_))

# Perplexity -> Lower the better
print("Model Perplexity: {} (lower is better)".format(best_lda_model.perplexity(tf)))

print("done in %0.3fs." % (time() - t0))

# <codecell> Print topics

print("\nTopics in LDA model:")
tf_feature_names = tf_vectorizer.get_feature_names()
print_top_words(best_lda_model, tf_feature_names, n_top_words)

# <codecell> eval on app

lda = best_lda_model
apps_df['topics'] = apps_df['description_processed'].map(
    lambda x: [t if t > 0.05 else 0 for t in lda.transform(tf_vectorizer.transform([x]))[0]])

# <codecell> store apps with topics

apps_df.to_pickle('data/processed/desc_lemma_user_topics.csv')

# <codecell> measure time

lda = best_lda_model
times = []
for index, row in apps_df.iterrows():
    start = datetime.datetime.now()
    d = [t if t > 0.05 else 0 for t in lda.transform(tf_vectorizer.transform([row['description_processed']]))[0]]

    times.append((datetime.datetime.now() - start).total_seconds() * 1000)

out = open('data/processed/topics_times.pickle', 'wb')
pickle.dump(times, out)
